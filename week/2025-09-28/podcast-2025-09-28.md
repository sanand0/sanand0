Alex: Hello and welcome to Anand’s Weekly Codecast for the week of 28 Sep 2025!
Maya: We’re Alex and Maya, and today we’ll walk you through the highlights of Anand’s commits this week.

Alex: Big week — Anand shipped a new set of small web apps and one shiny addition: a playlist builder called Findsongs. It takes a short description or a few example tracks, asks a model to return a strict JSON playlist, and lets you rate tracks, iterate, and copy or open results on YouTube.
Maya: That’s delightful and practical. The code shows he thought about reliable model output — he asks the LLM for a json_schema response so the page always receives a predictable object, then deduplicates and preserves liked songs when you refine the mix.
Alex: Why that matters: when you treat the model’s output as structured data rather than free text, UI code becomes simple and robust. Anand also added tests that fake model replies, so the UI is tested even without calling a real API.
Maya: Non-obvious takeaway: build the UX and the model contract together. If you design the schema first and write tests that assert it, you’ll avoid a lot of brittle parsing logic. Practical idea: reuse that pattern — a small JSON schema for any LLM response you rely on, and a test that simulates the model streams.

Alex: Staying with the tools collection, Anand also added a voice “Voicebot” demo — a small browser app that hooks to realtime LLM endpoints (WebRTC-style audio + datachannel), plus tests that stub peer connections. It’s a neat example of real-time speech-to-LLM UX without vendor SDKs.
Maya: The real lesson: voice interactions need both the audio path and a lightweight control channel for transcripts and events. He also included an API-token flow via the bootstrap LLM provider so the page can get a key without baking secrets into the site.
Alex: Practical idea: try a tiny voice experiment by wiring a microphone to a local model or proxy, and keep the control messages as structured JSON so you can react to partial transcripts.

Maya: The second big area was Anand’s personal scripts repo — a heavy burst of automation. The headline: better voice-to-text tooling and system automation. He added an ask script for recording voice notes, plus helper scripts to paste the result into the previously active window. Those connect to the LLM CLI and selectively copy only code blocks when the prompt mentions “code.”
Alex: He didn’t stop there — he created a set of systemd user services and timers (daily/weekly jobs) to update cached file lists, and refactored file-caching into a dedicated update-files script that writes to ~/.cache/sanand-scripts. That makes interactive pickers (rofi, fzf) fast even on big mounts.
Maya: Why it’s important: small, local automation that’s reliable and scheduled reduces the friction of repeated tasks. Non-obvious tip: use systemd user timers with Persistent=true so missed wakeups are caught up — great for laptops that sleep a lot.
Alex: Quick practical tweak you can copy: cache big directory listings into XDG cache and let your launcher read the cache instead of scanning disk on every invocation — big speedup.

Maya: Another big set of changes: the public course materials Anand maintains for the “Tools in Data Science” class. He reordered modules — deployment before AI coding — and massively expanded the AI-coding material: tests, tools, and prompts.
Alex: That’s an intentional curricular move: teach deployment before asking students to ship AI-coded projects. He also added concrete guidance on AI-code testing — scorecard-driven evals, premortem tests, property-based fuzzing, mutation testing, and agent-safe tooling.
Maya: Why this matters: AI-generated code scales fast, and without good tests and gating you get fragile repos. The non-obvious point: teach students to ask for failing tests first — make the tests the specification — then have agents implement until the tests pass.
Alex: Practical idea: if you’re using LLMs in a project, add a tiny “premortem test” that encodes the single most catastrophic failure case, and require it before merging.

Alex: On the docs side for that course, he also added a Hugging Face Spaces + Docker guide and a detailed student feedback report from May 2025 — so the course updates are evidence-driven.
Maya: Concrete effect: clearer expectations for students, better syllabus sequencing, and new practical modules (AI coding tools and tests) that are ready-to-use. If you teach or onboard others, swap “deployment before features” where possible. It pays off.

Maya: Over in data storytelling, Anand added a full project: a “Bollywood Box Office Champions” interactive visualization. He scraped Wikipedia into CSV, added inflation adjustments, and wired a D3 bubble chart with hover/click details and a toggle to show inflation-adjusted grosses.
Alex: That’s a lovely example of end-to-end data work — scrape, clean, document provenance, and make an explorable chart. He included the scraper script and a detailed README explaining sources and assumptions.
Maya: Non-obvious takeaway: when you present dollar—or rupee—numbers across decades, do inflation adjustment as a normal step. And document how you parsed messy strings from Wikipedia: it saves debugging later.

Alex: The prompts repo got polished too — more prompts, metadata like “purpose” at the top of files, and a new “chatgpt custom instructions” prompt. That makes prompts discoverable and reusable.
Maya: Little thing, big ROI: add a one-line purpose field to each prompt file — it’s easier to pick the right prompt without opening it. Practical idea: start your own prompt library with tiny metadata so teammates can reuse them.

Maya: Anand also pushed updates to the generative WhatsApp podcast automator. He’s been iterating the pipeline that reads a WhatsApp JSON, builds weekly threads, generates a two-host script via an LLM, and synthesizes TTS segments into an MP3.
Alex: The repo now includes per-week podcast markdown, TTS segments, and a tested flow for script generation — a real example of turning chat transcripts into publishable audio. Useful if you want an automated weekly digest of a chat group.
Maya: Non-obvious point: when you automate creative outputs, keep each step as a file artifact (transcript → script → audio chunks) so you can re-run only parts and audit costs.

Alex: A couple of smaller but neat things: the “developer styles” experiments in the llm-evals area, where Anand had models produce short checkout state machines in the style of many known developers — great for learning patterns; and in the aipe repo he made config.example.js the canonical template so people don’t accidentally commit secrets.
Maya: Those are great housekeeping moves: templates for config prevent leaks, and “style mimic” is a clever way to learn idiomatic patterns from many authors quickly.

Alex: Okay, listener tip time. Quick, actionable take from me: if you use LLMs to generate structured output, define a JSON schema first, write a test that simulates a model reply, and fail-fast on schema violations. Maya, how would you apply that this week?
Maya: I’d add a tiny schema for our internal FAQ generator and a CI check that pulls a canned model response and validates it before merge. That way the content team can iterate prompts but the consumers (the website) stay robust.

Maya: My tip: use systemd user timers for scheduled personal tasks instead of cron — make them Persistent=true so they run after sleep, and keep the jobs idempotent (e.g., update-file caches). Alex, how would you use that?
Alex: I’ll schedule a nightly index of my notes and cache results to ~/.cache, then wire my launcher to read that cache. That will make my fuzzy file picker instant.

Alex: That’s all for this week. Thanks for listening.
Maya: Thanks — have a great week, and see you next time on Anand’s Weekly Codecast!
Alex: Goodbye!
Maya: Goodbye!
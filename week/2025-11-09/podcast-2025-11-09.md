Alex: Hello and welcome to Anand’s Weekly Codecast for the week of 09 November 2025!
Maya: We’re Alex and Maya, and today we’ll walk you through the highlights of Anand’s commits this week.

Alex: Big week on the course front. Anand updated the public course materials for the Tools in Data Science class — added a firm live evaluation time for Project 2, dropped a pile of FAQ-style transcripts from live sessions, and rewrote the little shell tool that turns YouTube sessions into FAQ transcripts.
Maya: That shell tool rewrite is the quiet hero here. Anand added explicit chunking of long videos, a short overlap to avoid clipped audio, caching so reruns are fast, and clearer usage docs. What does that actually do for students?
Alex: Practically, it means long tutorial videos stop failing when they’re too large for the transcription step. Chunking with overlap keeps sentences from being chopped, caching saves money and time when you re-run the pipeline, and the FAQ transcripts make lecture content searchable and reusable.
Maya: Why that matters beyond this class: if you’re processing any long media — lectures, meetings, webinars — you hit limits in tools and models. The pattern Anand used is useful: split long inputs into ~1 hour chunks, small overlaps, name chunks predictably, cache results, and keep a playlist manifest so you can rerun only the missing pieces.
Alex: Non-obvious takeaway: when you automate media work, design for incremental re-runs. Don’t rerun the whole pipeline because one chunk failed — keep a cache manifest and chunk-level outputs named by date+id so recovery is cheap. Practical idea: apply the same chunk + overlap + cache pattern when sending long docs to any LLM that has token limits.

Maya: Relatedly, Anand added a prompt and a plan page describing how to improve that FAQ script — step-by-step notes, token usage comments, and instructions to automatically create .opus and .md files per chunk. That kind of internal prompt-as-spec is a great way to make future edits safer.
Alex: Yes — documenting the prompt that produced the edits is like shipping a mini spec for future maintainers. If you’re doing any LLM-assisted coding, save the prompt that created the change.

Maya: Next big area: scraping and archiving discussion forums. Anand published a full Discourse-thread scraper — both as a browser bookmarklet in the tools collection and as a standalone CLI project that calls the Discourse API, writes one JSON per thread, and a companion script that turns those JSON files into neat Markdown mirrors.
Alex: That’s huge for courses and archival work. Instead of losing a week’s worth of Q&A buried in a forum, you can snapshot topics as structured JSON, produce readable Markdown, and feed those into search tools or LLM pipelines.
Maya: Practical effects: easier offline review, reproducible LLM prompts over a thread, and the ability to query or summarize historical discussion. Non-obvious takeaway: scrape as JSON first and keep that as the canonical archive; generate human-facing outputs (Markdown, summaries) lazily from the JSON so you can reprocess with improved parsers later.
Alex: And a nice engineering touch — the bookmarklet approach works around client-rendered UIs where curl fails, while the CLI uses the official API when you have credentials. If you plan to build course analytics, this is an immediate data source.

Maya: Shifting gears to talks: Anand added two new talk pages — a full deck and transcript for an “LLM Psychology” podcast episode and a detailed talk about Diagram Chasing and making open data useful. He also fixed a bunch of slide links and errata.
Alex: For listeners that build talk assets: those commits show a tidy way to manage talk materials — Marp slides, transcripts, QR links, and a short feedback checklist. The practical effect is better discoverability and reliable links for old slides.
Maya: Non-obvious tip: date-prefix release files (e.g., 2024-08-10-...) for slide assets so links stay stable across releases. Also, CC0 transcripts make it safe to reuse examples in teaching.

Alex: Another neat story: Anand analyzed GitHub Copilot CLI logs from student runs and turned that into a data story. He published a short analysis that shows environment mix (lots of Windows), model choices students used, and verification practices.
Maya: That’s an actionable dataset for instructors. If most students use Windows and a specific model, design labs and scripts to match that default. And he suggests requiring a simple verification banner in student outputs — a one-line proof so graders get consistent automation.
Alex: Non-obvious takeaway: small grading requirements (a 2-line verification banner) dramatically reduce friction in automated grading pipelines. If you teach, lock a tiny machine-checkable artifact into the spec.

Maya: On the product side, Anand improved a policy-validation web app: he added a “generate sample documents” button that makes two short example docs — likely pass and likely fail — using the current rule set, and moved a destructive storage-clear button into a less-hidden location next to ingest/consolidate.
Alex: That’s a great usability win. People often don’t know how to exercise validation rules; generating a positive and negative sample instantly surfaces gaps in rule-extraction. Also, moving the clear button into the main row makes the action discoverable but still obvious — less chance of surprising users.
Maya: Non-obvious idea: always give users a failing example early — it helps them tune their rules and prevents the “it says everything passes” problem.

Alex: On the developer tooling front, Anand added multiple improvements to his scripts and contributors’ workflow: pre-commit hooks, a dev smoke-test script that checks the container and common tools, filter/truncate options for session listings, and a brief guideline to cache LLM/API/HTTP results during loops.
Maya: Those are reliability and onboarding wins. The dev.test.sh means new contributors can run a single script to sanity-check their environment. The caching guideline is important — agents looping over similar inputs can otherwise explode API cost and latency.
Alex: Non-obvious takeaway: put small, focused smoke tests in repo roots — they pay back when CI flakes or new machines are provisioned. And when you use LLMs in loops, write results to .cache/ with a key that includes the prompt and inputs — you’ll cut cost drastically.

Maya: On the stats tooling side, Anand updated a small data-science tool to show p-values in the UI and allow re-testing hypotheses. So users now see a numeric p-value and can re-run tests from the UI.
Alex: That’s helpful — p-value in one phrase: it’s the probability of seeing data at least as extreme as observed if the null hypothesis is true. Showing the number, plus an interpretation, helps people avoid magic black boxes.
Maya: Practical idea: always show p-values with a plain-language sentence: “p = 0.03 — this suggests the observed effect is unlikely under the null hypothesis (about 3%).” Numbers plus interpretation beat raw stat dumps.

Alex: A small but important permission change: the API playground that talks to Google Workspace now includes calendar write scopes and updated prompt text. So the agent can now create or modify calendar events, not just read them.
Maya: That opens up real automations — the app can schedule, update, or delete events on behalf of users. But remember: write scopes require careful prompts and explicit user consent. Non-obvious: when you enable write access, log intent and provide an undo path — users expect reversible actions.

Maya: A cluster of smaller polish items: Anand tweaked a trending-repos UI to rename status labels to more actionable verbs, added a link to “Terminal to HTML” tools, and recommended a maintained SSE parser in place of his tiny package.
Alex: Those are usability and maintenance wins. Renaming statuses to verbs helps people act faster; pointing folks to a maintained parse-sse reduces maintenance burden; and linking to a trusted external tool is honest curation.

Maya: On LLM pricing and audio: Anand refreshed his LLM frontier data and published a focused note on OpenAI TTS costs. He measured real requests, compared models, and concluded TTS-1 is a decent cost/quality point for podcast-style audio.
Alex: Practical effect: if you’re producing constant audio, these per-minute math and sample measurements help pick a voice model that balances cost versus fidelity. Non-obvious find: input tokens to TTS can multiply into many audio tokens — measure end-to-end cost before choosing the default.

Maya: Anand also updated the weekly group podcast generator: changed date formatting and switched the TTS model used for rendering voices to a different TTS engine. Small, but it affects final audio size, cost, and voice quality.
Alex: That’s the sort of engineering detail that saves money across weekly runs. If you produce audio at scale, choose a TTS that hits your quality target at the lowest cost and test a representative episode before locking it in.

Maya: On testing and infra: Anand refactored an internal test suite for his API-proxy project, moving many tests into a modern workers+vitest setup and adding helpers. That’s a large rewrite of test scaffolding.
Alex: The practical effect: faster, more robust CI, clearer test helpers, and better isolation for Cloudflare worker-style tests. For maintainers, good test infra reduces fear of refactors.

Maya: One last note: he published a new tool in the toolkit — a Discourse bookmarklet entry, sample fixtures, and a packaged minified script, plus tests. That ties back to the scraping work but also shows a full tool lifecycle: UI page, min build, tests, and docs.
Alex: A good reminder — when you release small web tools, ship a tiny index page, tests, and an example HTML fixture. It reduces friction for users and reviewers.

Maya: Listener tips time. Quick and actionable — one each, and then ask the other how they’d apply it.
Alex: Tip one: If you process long audio or long documents with LLMs, always chunk with a small overlap and keep a chunk-level cache manifest. That way, failed chunks are re-runnable and cached outputs let you iterate cheaply. Maya — where would you use that this week?
Maya: I’d use it to make meeting notes reliable: chunk recordings at 20–30 minute intervals with 10s overlap, transcribe each chunk, then stitch summaries. That reduces token spikes and makes manual edits local to one chunk.

Maya: My tip: when you add write permissions to an app (calendar, drive), add lightweight undo and an activity log view that shows recent automated changes. Make “revert last 5 changes” possible from the UI. Alex — how would you apply that in a student project?
Alex: I’d add an “audit” endpoint and a one-click revert that re-posts a prior version. For a course, that means students can experiment with calendar automation without risking lost demo slots — and graders can see what the app actually did.

Alex: And one bonus tip from our discussions — when you use LLMs to generate negative test cases (failing examples), keep both a pass and fail sample in your repo. It’ll catch brittle rules and teach your validator what “bad” looks like.
Maya: I’ll apply that to the policy validation UI — generate a failing doc and wire it to one-click download in the UI so users can run the validator right away.

Alex: That’s all from us for this week. Thanks to Anand for the steady stream of practical fixes and people-focused improvements.
Maya: See you next week. Keep experimenting, write down the tiny operational rules that save time, and don’t forget to cache the expensive stuff. Goodbye for now!
Alex: Goodbye!
Maya: Bye!
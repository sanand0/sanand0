Alex: Hello and welcome to Anand’s Weekly Codecast for the week of 02 Nov 2025!
Maya: We’re Alex and Maya, and today we’ll walk you through the highlights of Anand’s commits this week.

Alex: Big one to start: Anand updated a public course that teaches tools in data science. He added a fleshed‑out Project 2, rewrote an entire module around what he calls “vibe analysis” (an outcome‑focused way to use LLMs), and added practical tutorials—LLM-powered website scraping and a Datasette guide for publishing and exploring data.
Maya: That’s a lot. What should listeners understand about the Project 2 changes?
Alex: Practically, students now have a clear quiz system: they register via a Google form, submit an API endpoint that will accept quiz tasks, and provide two prompts — a system prompt (meant to prevent revealing a secret code word) and a user prompt (meant to try to override it). Anand documented how prompts will be tested against each other, required HTTPS for endpoints, and added a demo endpoint you can hit to try your implementation.
Maya: Why does that matter beyond class logistics?
Alex: It’s a small lesson in building resilient, auditable agent services. The project forces students to think about authentication (a secret string), defensive endpoint behavior (return 403 on bad secrets), and reproducible prompt-testing setups. Those exact same concerns show up in production when agents call webhooks or when you grade model behavior automatically.
Maya: Any non-obvious takeaways?
Alex: Two. One, explicitly designing tests that mix system and user prompts is a great way to evaluate prompt robustness—don’t just test prompts in isolation. Two, require an MIT license and a public repo at evaluation time: it dramatically simplifies reproducibility and grader automation.
Maya: Practical idea from this: if you’re building an agent that accepts external tasks, start by offering a demo URL and a small sample payload so integrators can smoke test without deploying full infra.
Alex: And a quick checklist: verify secrets server‑side, return 403 on mismatch, and log session resets so you can debug prompt interactions.

Alex: Next up — design and slides. Anand shipped a new Marp theme and a number of refinements: layout utilities to justify multi‑column content, font sizing utilities, new CSS variables for consistent styling, and a small but very useful <transcript> tag that hides content from slide rendering while keeping it visible in raw Markdown.
Maya: Tell me why the <transcript> tag is useful in practice.
Alex: It’s simple but powerful: you can embed speaker notes, full transcripts, or references in the same Markdown file, have them show up on GitHub or in a README, but keep slides visually clean when rendered with Marp. That helps accessibility and keeps a single source of truth for slides and notes.
Maya: And the layout utilities—what do they buy you?
Alex: A lot of polish with little effort. The columns-justify utility auto aligns first element left, last right, and centers the rest. That makes headers and footers with mixed elements (title, nav, icons) look balanced without custom CSS per slide. He also added small font classes (.small, .small-1, etc.) and some build fixes for the GitHub Actions workflow so the site actually generates with the theme.
Maya: Non-obvious tip: centralizing spacing and font sizes into variables means you can rebrand a whole slide deck with only a few edits—great for templates and for keeping consistency across talks.
Alex: Practical idea: put long speaker notes or full transcripts inside <transcript> and link them in your show notes; you get both presenter help and public transcripts without extra files.

Alex: Closely related: Anand updated his talks repo to use that theme and refactored the build process. He added a setup script that downloads the theme and runs Marp across all slide directories, and he ignored the downloaded theme in git so it’s always fetched during build.
Maya: Why split out the build into a script?
Alex: Maintainability. The script centralizes logic (download theme, iterate directories, build if README.md changed), which is easier to debug and update than a giant inline npm command. It’s also safer for CI and for anyone cloning the repo: run setup.sh and you get a reproducible build.
Maya: There was also a major new talk on LLM data visualization, right?
Alex: Yes—lots of code, notebooks, and reproducible demos: SOM demos, UMAP-based topography visualizations, novel “river flow” and “topography” metaphors, synthetic data generators and classroom-ready materials. For instructors and practitioners this is an immediately usable teaching pack.

Alex: On the tooling front, Anand made a large set of changes to his developer scripts and agent tooling. The Gmail CLI was rewritten to an async implementation that streams results, emits JSONL, and adds a --reauth option. He improved OAuth handling to cope with refresh failures, added lots of small shell helpers, and beefed up a dev container script so it supports GPUs, SSH agent forwarding, and many useful mounts.
Maya: That sounds like infrastructure polish, but what’s the practical effect for daily work?
Alex: Better ergonomics and reliability. The Gmail CLI now streams message details rather than buffering, which is much friendlier in pipelines. The OAuth changes detect refresh errors and fall back to re‑auth flow instead of silently failing. The dev container now makes it easy to test GPU workloads and gives consistent mounts for agent credentials—huge for reproducible development of LLM tooling.
Maya: Any subtle warnings?
Alex: A couple. First, when you make a CLI produce JSONL you enable much better composability, but be sure consumers handle one-JSON-per-line. Second, when you mount lots of host config into containers, treat secrets carefully—don’t mount production credentials into ephemeral dev containers.

Alex: Anand also added a new web tool: an X (Twitter) thread scraper bookmarklet. It scrapes a tweet and its replies into JSON, filters out promoted tweets, and computes two handy scores—“buzz” (short‑term engagement signals like reposts and likes) and “keep” (longer‑term signals like bookmarks and replies). He included unit tests, a fixture HTML file, a verify script that uses Chrome Remote Debugging for in‑browser checks, and an index page that builds the bookmarklet.
Maya: So it’s not just another scraper—there’s scoring and verification.
Alex: Exactly. The buzz/keep scores help you prioritize which replies or threads to keep for research. The verify.mjs script uses the Chrome DevTools Protocol so you can inject and validate the bookmarklet against real tabs automatically—great for trustworthiness.
Maya: Non-obvious tip: because X changes DOM structure often, having unit tests and a fixture saves a ton of debugging time when the site shifts.
Alex: Practical idea: use the bookmarklet to collect threads for qualitative research, then run the scoring to find the 10 posts worth saving and feed them into a downstream analysis pipeline.

Alex: In data experiments, Anand published a careful prompt‑caching experiment for OpenAI. He wrote scripts to run, log, and analyze caching behavior and produced a short report: caching works but has variable lag and TTL. Multi‑message prompts sometimes got larger cached slices; single‑message prompts sometimes missed the immediate repeat and warmed up after ~60 seconds. TTLs varied too.
Maya: That’s a useful empirical result. How should people interpret this?
Alex: If your system assumes deterministic prompt caching you’ll be surprised. The safe pattern is: 1) treat caching as a best‑effort performance boost, not a guaranteed speedup; 2) warm up a prompt by repeating it once before the real work if latency matters; 3) log cache headers if your provider exposes them.
Maya: Practical takeaway: for latency‑sensitive flows, design a warm‑up or short retry strategy and measure per‑region behavior since caches and routing can vary.

Alex: A few smaller but helpful updates: Anand regenerated a weekly podcast episode script in the generative‑AI group project, updated some tutorial config examples to add provider profiles (Gemini, Azure, OpenRouter), and improved documentation for a video personalization demo.
Maya: All the usual polishing that keeps a collection of tools usable—documentation, examples, and small bug fixes.

Alex: Time for listener tips. I’ll go first: when you expose an HTTP endpoint for agents or graders, always include a small demo payload and a clear failure mode—return 403 for bad secrets, 400 for malformed payloads, and 200 for accepted tasks with a concise JSON response. Maya, how would you apply that?
Maya: I’d wire that into CI: add a smoke test that POSTs the demo payload to the deployed endpoint and asserts a 200 and a known JSON schema. That catches runtime config or TLS problems before grading or production traffic hits the service.

Maya: My tip: if you rely on prompt caching to save cost or latency, instrument and measure it—log cached_token counts and elapsed time on warm vs cold requests. Treat caching as probabilistic and build a short warm-up pass if you need consistent latency. Alex, how would you use that?
Alex: I’d add a middleware that automatically issues a tiny, free warm-up request for critical prefixes after deployment or when a new prompt is first used, and expose a dashboard showing cache hit ratio and median latency per prompt.

Alex: One more quick tip—use the <transcript> tag in your slides to publish transcripts and long notes alongside slides so people can read them in the repo without cluttering slides. Maya, would you use that?
Maya: Absolutely. I’d put full meeting transcripts and speaker notes there and link them from the talk page—searchable, accessible, and single‑file source control.

Alex: That’s it for this week. Thanks to Anand for the steady stream of practical updates—course material, theme work, developer tooling, and reproducible experiments.
Maya: See you next week. Keep experimenting, test your assumptions, and remember to log the things you care about.
Alex: Goodbye for the week!
Maya: Bye — and happy building!